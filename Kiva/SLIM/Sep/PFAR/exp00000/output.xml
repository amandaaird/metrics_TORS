<experiment count="9">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>l1-reg</name>
      <value>0.6567319092645576</value>
    </param>
    <param>
      <name>l2-reg</name>
      <value>1.0495346406822583</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.6379622764799714</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>9</exp_no>
      <date>2022-12-05 00:12:36.430272</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 00:14:35.367393</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 00:22:18.802535</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-05 00:25:11.202478</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 00:25:25.902695</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 00:26:27.229751</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 16:52:23.434969</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 16:52:34.341221</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 16:59:41.599833</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.008215760969927894</metric>
        <metric name="PStatisticalParityEvaluator">-0.19073881373568613</metric>
        <metric name="kendall_tau.py">0.24921625745595782</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6607571587103421</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.008536599287701856</metric>
        <metric name="PStatisticalParityEvaluator">-0.19604886924876075</metric>
        <metric name="kendall_tau.py">0.25806075738660006</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6736651903374749</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.007017670441746716</metric>
        <metric name="PStatisticalParityEvaluator">-0.19363256784968225</metric>
        <metric name="kendall_tau.py">0.2554307116104869</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6654016011731367</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.007835664386902532</metric>
        <metric name="PStatisticalParityEvaluator">-0.1918240620957251</metric>
        <metric name="kendall_tau.py">0.25685115827437927</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.66633468085528</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.008185516385004053</metric>
        <metric name="PStatisticalParityEvaluator">-0.1947177628171895</metric>
        <metric name="kendall_tau.py">0.2481065334997919</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6632138002100547</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
