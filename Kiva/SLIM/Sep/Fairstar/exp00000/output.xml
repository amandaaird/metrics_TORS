<experiment count="9">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>l1-reg</name>
      <value>0.9731907679617346</value>
    </param>
    <param>
      <name>l2-reg</name>
      <value>1.1095471926792866</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.3090858128554729</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>9</exp_no>
      <date>2022-12-05 01:10:13.213191</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 01:13:23.937064</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 01:20:31.468302</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-05 01:28:22.622767</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 01:28:40.526959</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 01:29:45.282818</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 16:30:53.846607</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 16:31:04.291354</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 16:39:04.375582</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.0060182104041725006</metric>
        <metric name="PStatisticalParityEvaluator">0.14588969823098918</metric>
        <metric name="kendall_tau.py">-0.07922874185046468</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.249796633176783</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.006757111871118884</metric>
        <metric name="PStatisticalParityEvaluator">0.15243046529761678</metric>
        <metric name="kendall_tau.py">-0.08046053544180883</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.26028245447159304</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.005990127815850992</metric>
        <metric name="PStatisticalParityEvaluator">0.1486951983298354</metric>
        <metric name="kendall_tau.py">-0.08014981273408238</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.2479304639042467</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.005724518974155693</metric>
        <metric name="PStatisticalParityEvaluator">0.1589133247089063</metric>
        <metric name="kendall_tau.py">-0.08286863642668883</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.24924540752630642</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.0070117534978733865</metric>
        <metric name="PStatisticalParityEvaluator">0.1474365613671485</metric>
        <metric name="kendall_tau.py">-0.08263559439589402</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.2454302756475041</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
