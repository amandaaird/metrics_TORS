<experiment count="3">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>l1-reg</name>
      <value>0.6006927636514854</value>
    </param>
    <param>
      <name>l2-reg</name>
      <value>0.41548041168751126</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.9263164423659205</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>3</exp_no>
      <date>2022-12-04 22:49:44.926609</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-04 22:52:50.542982</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-04 22:58:57.382837</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-04 23:01:51.897552</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-04 23:02:08.777033</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-04 23:03:22.498921</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 16:42:52.247620</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 16:43:02.589367</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 16:50:29.735551</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.008076156663552941</metric>
        <metric name="PStatisticalParityEvaluator">-0.09141519250779151</metric>
        <metric name="kendall_tau.py">0.1392315161603551</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.5611619701563522</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.008351227994593981</metric>
        <metric name="PStatisticalParityEvaluator">-0.08609305952688728</metric>
        <metric name="kendall_tau.py">0.15172700790678317</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.5697979608822306</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.006919236644935475</metric>
        <metric name="PStatisticalParityEvaluator">-0.0874217118997746</metric>
        <metric name="kendall_tau.py">0.14310445276737407</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.564179061886927</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.007462238391738345</metric>
        <metric name="PStatisticalParityEvaluator">-0.08646830530399358</metric>
        <metric name="kendall_tau.py">0.13358302122347065</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.5616966391217327</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.007784482198858901</metric>
        <metric name="PStatisticalParityEvaluator">-0.08855515277056952</metric>
        <metric name="kendall_tau.py">0.14048550423082257</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.5634892792739234</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
