<experiment count="48">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>l1-reg</name>
      <value>0.5708605705349499</value>
    </param>
    <param>
      <name>l2-reg</name>
      <value>0.17898997806223732</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.06485786767404889</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>48</exp_no>
      <date>2022-12-05 09:02:03.096362</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 09:03:57.835018</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 09:11:28.954884</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-05 09:14:24.206503</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 09:14:38.876935</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 09:15:36.412226</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 15:42:28.474799</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 15:42:39.402706</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 15:50:14.622573</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.008549171366809223</metric>
        <metric name="PStatisticalParityEvaluator">-0.3991675338189447</metric>
        <metric name="kendall_tau.py">0.5780052711887917</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7866835701404988</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.00818022804823932</metric>
        <metric name="PStatisticalParityEvaluator">-0.40374317650117686</metric>
        <metric name="kendall_tau.py">0.6148592037730614</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7983105939004815</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.006868803600517617</metric>
        <metric name="PStatisticalParityEvaluator">-0.4010960334029294</metric>
        <metric name="kendall_tau.py">0.5937522541267858</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7896831143610169</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.008008818263262057</metric>
        <metric name="PStatisticalParityEvaluator">-0.40553686934023947</metric>
        <metric name="kendall_tau.py">0.6023415175475101</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7940506806967481</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.00822251464780135</metric>
        <metric name="PStatisticalParityEvaluator">-0.40129466597618524</metric>
        <metric name="kendall_tau.py">0.5848078790400888</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7892407902819887</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
