<experiment count="38">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>l1-reg</name>
      <value>0.5687401471098481</value>
    </param>
    <param>
      <name>l2-reg</name>
      <value>1.2953501754337662</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.15480565903747856</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>38</exp_no>
      <date>2022-12-05 10:23:42.319604</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 10:25:21.670156</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 10:30:06.716590</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-05 10:36:08.357023</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-05 10:36:19.033704</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-05 10:37:14.356868</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 16:20:17.117148</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 16:20:27.379478</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 16:28:34.501817</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.0077786057977861445</metric>
        <metric name="PStatisticalParityEvaluator">-0.19276795005203323</metric>
        <metric name="kendall_tau.py">0.002014148980441118</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.46846716406079697</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.008645499824421442</metric>
        <metric name="PStatisticalParityEvaluator">-0.20077982843774594</metric>
        <metric name="kendall_tau.py">0.019680954362602303</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.4891265977052494</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.007791812756143819</metric>
        <metric name="PStatisticalParityEvaluator">-0.19133611691023134</metric>
        <metric name="kendall_tau.py">-0.004400055486197804</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.4739321086736817</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.006569833824742965</metric>
        <metric name="PStatisticalParityEvaluator">-0.19896507115136342</metric>
        <metric name="kendall_tau.py">0.005221251213760579</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.4757557021976498</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.008270330600546126</metric>
        <metric name="PStatisticalParityEvaluator">-0.19549456240290453</metric>
        <metric name="kendall_tau.py">0.012678596199195449</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.4767322394625765</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
