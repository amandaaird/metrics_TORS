<experiment count="9">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>lambda</name>
      <value>0.9</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>9</exp_no>
      <date>2022-12-12 03:33:56.952719</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-12 03:34:30.128489</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-12 05:06:11.086272</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-12 05:06:35.501307</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-12 05:44:27.542140</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-12-12 05:54:29.196946</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-12-12 05:54:42.806654</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-12-12 06:20:49.091283</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 12:14:29.404863</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 12:14:39.492609</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 12:22:08.964110</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.013481033950763862</metric>
        <metric name="PStatisticalParityEvaluator">-0.22086368366279913</metric>
        <metric name="kendall_tau.py">-0.006564017200721322</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.5953600261577788</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.01628580649047497</metric>
        <metric name="PStatisticalParityEvaluator">-0.21388094619178194</metric>
        <metric name="kendall_tau.py">-0.06334859203773062</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6060004161464834</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.01976795053439118</metric>
        <metric name="PStatisticalParityEvaluator">-0.3957202505219231</metric>
        <metric name="kendall_tau.py">-0.019703148841725616</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6065905514931731</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.017371835504207084</metric>
        <metric name="PStatisticalParityEvaluator">-0.20507115135828302</metric>
        <metric name="kendall_tau.py">-0.12868913857677902</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6058947248479081</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.017923703153675412</metric>
        <metric name="PStatisticalParityEvaluator">-0.20875194199890507</metric>
        <metric name="kendall_tau.py">-0.16141489804411152</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.6182617264134117</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
