<experiment count="41">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>user-reg</name>
      <value>0.18514092810940633</value>
    </param>
    <param>
      <name>item-reg</name>
      <value>0.17461415292138982</value>
    </param>
    <param>
      <name>bias-reg</name>
      <value>0.18828028072559394</value>
    </param>
    <param>
      <name>num-factors</name>
      <value>95</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.08928030201323957</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>41</exp_no>
      <date>2022-11-11 14:05:46.522769</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-11 14:06:09.191857</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-11 14:10:29.343969</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-11-11 14:16:26.074849</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-11 14:16:34.732464</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-11 14:17:24.991241</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 11:34:30.395608</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 11:34:41.222649</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 11:42:26.383238</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.01791962860750077</metric>
        <metric name="PStatisticalParityEvaluator">-0.7998439125911052</metric>
        <metric name="kendall_tau.py">-0.2218837564155916</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7071445712700394</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.021091916281594663</metric>
        <metric name="PStatisticalParityEvaluator">-0.8000000000000543</metric>
        <metric name="kendall_tau.py">0.2216507143847968</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7071109426708677</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.02489808845718355</metric>
        <metric name="PStatisticalParityEvaluator">-0.8000000000000543</metric>
        <metric name="kendall_tau.py">0.19074490220557638</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7071031746031745</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.023390161283569785</metric>
        <metric name="PStatisticalParityEvaluator">-0.8000000000000544</metric>
        <metric name="kendall_tau.py">-0.3062671660424469</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7071084457919664</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.022231505989432424</metric>
        <metric name="PStatisticalParityEvaluator">-0.8000000000000544</metric>
        <metric name="kendall_tau.py">-0.5108003884033846</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7073625230366803</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
