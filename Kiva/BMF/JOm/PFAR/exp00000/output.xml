<experiment count="41">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>user-reg</name>
      <value>0.06739319494315868</value>
    </param>
    <param>
      <name>item-reg</name>
      <value>0.053665874937364355</value>
    </param>
    <param>
      <name>bias-reg</name>
      <value>0.0767029780276064</value>
    </param>
    <param>
      <name>num-factors</name>
      <value>28</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.013902606385323832</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.512212259379621</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>41</exp_no>
      <date>2022-11-28 05:51:49.472995</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 05:52:07.091942</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 05:58:05.538072</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-11-28 06:01:14.887876</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 06:01:24.480777</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 06:02:20.716261</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 11:48:19.520526</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 11:48:30.199153</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 11:55:55.175316</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.011728547623383311</metric>
        <metric name="PStatisticalParityEvaluator">-0.9994797086368367</metric>
        <metric name="kendall_tau.py">0.9999999999999998</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">1.0</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.013283445433725912</metric>
        <metric name="PStatisticalParityEvaluator">-1.0</metric>
        <metric name="kendall_tau.py">0.9999445138021915</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.9999950062421973</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.014620855907441279</metric>
        <metric name="PStatisticalParityEvaluator">-0.9990605427974951</metric>
        <metric name="kendall_tau.py">0.9999999999999998</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">1.0</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.014048455065985734</metric>
        <metric name="PStatisticalParityEvaluator">-1.0</metric>
        <metric name="kendall_tau.py">0.9999999999999998</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">1.0</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.013515674391990076</metric>
        <metric name="PStatisticalParityEvaluator">-1.0</metric>
        <metric name="kendall_tau.py">0.9997558607296434</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.9999751798347304</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
