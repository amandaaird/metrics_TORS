<experiment count="30">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>user-reg</name>
      <value>0.18264597853745373</value>
    </param>
    <param>
      <name>item-reg</name>
      <value>0.09728633955171251</value>
    </param>
    <param>
      <name>bias-reg</name>
      <value>0.04526074745494704</value>
    </param>
    <param>
      <name>num-factors</name>
      <value>57</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.4191103646238945</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>30</exp_no>
      <date>2022-11-28 06:29:49.404539</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 06:30:30.490479</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 06:36:20.546917</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-11-28 06:38:48.899187</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 06:38:58.539293</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 06:39:54.337708</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 10:56:18.553842</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 10:56:32.031534</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 11:05:17.043935</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.013345928603906038</metric>
        <metric name="PStatisticalParityEvaluator">-0.8026534859521868</metric>
        <metric name="kendall_tau.py">0.4087834651130532</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.9263891365951292</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.016056454778512037</metric>
        <metric name="PStatisticalParityEvaluator">-0.7901741616844834</metric>
        <metric name="kendall_tau.py">0.27606047995561106</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.9428104353684877</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.017195712042786714</metric>
        <metric name="PStatisticalParityEvaluator">-0.8083507306889863</metric>
        <metric name="kendall_tau.py">0.8310001387154944</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.986390176961338</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.017257056604906642</metric>
        <metric name="PStatisticalParityEvaluator">-0.7926002587322657</metric>
        <metric name="kendall_tau.py">0.46198363157164657</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.9577920753819631</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.013813292916283769</metric>
        <metric name="PStatisticalParityEvaluator">-0.958052822371841</metric>
        <metric name="kendall_tau.py">0.266461367734776</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.8807148405762637</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
