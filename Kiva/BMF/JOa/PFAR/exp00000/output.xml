<experiment count="39">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>user-reg</name>
      <value>0.024986166998251597</value>
    </param>
    <param>
      <name>item-reg</name>
      <value>0.0686043866735774</value>
    </param>
    <param>
      <name>bias-reg</name>
      <value>0.13125354819681567</value>
    </param>
    <param>
      <name>num-factors</name>
      <value>29</value>
    </param>
    <param>
      <name>lambda</name>
      <value>0.8273437035184119</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.8762703795023521</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>39</exp_no>
      <date>2022-11-28 05:30:41.129169</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 05:30:58.708200</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 05:36:45.240369</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-11-28 05:40:12.178801</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-28 05:40:24.414096</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-28 05:41:20.556365</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 11:21:14.816863</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 11:21:26.361686</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 11:29:09.273080</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.01450108141515913</metric>
        <metric name="PStatisticalParityEvaluator">-0.8909469302809766</metric>
        <metric name="kendall_tau.py">0.2943265362741018</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.8007401561540137</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.016557909530962298</metric>
        <metric name="PStatisticalParityEvaluator">-0.8936314010917793</metric>
        <metric name="kendall_tau.py">0.49423220973782767</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.7986164219329013</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.017597655548063147</metric>
        <metric name="PStatisticalParityEvaluator">-0.8936325678497063</metric>
        <metric name="kendall_tau.py">0.42272159800249687</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.8052865366704318</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.017476608534551914</metric>
        <metric name="PStatisticalParityEvaluator">-0.8934023285899291</metric>
        <metric name="kendall_tau.py">0.335685948120405</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.8005696549947486</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.016614253900403476</metric>
        <metric name="PStatisticalParityEvaluator">-0.9003107198343006</metric>
        <metric name="kendall_tau.py">0.0912026633374948</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.8018795553177576</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
