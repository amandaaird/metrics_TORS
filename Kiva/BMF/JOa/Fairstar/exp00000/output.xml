<experiment count="49">
  <!--DO NOT EDIT. File automatically generated by librec-auto-->
  <meta>
    <param>
      <name>user-reg</name>
      <value>0.18814428948663384</value>
    </param>
    <param>
      <name>item-reg</name>
      <value>0.19468801645537148</value>
    </param>
    <param>
      <name>bias-reg</name>
      <value>0.1664286812805778</value>
    </param>
    <param>
      <name>num-factors</name>
      <value>108</value>
    </param>
    <param>
      <name>alpha</name>
      <value>0.23822789369898545</value>
    </param>
  </meta>
  <statuses>
    <status>
      <message>Executing</message>
      <exp_no>49</exp_no>
      <date>2022-11-14 04:48:38.002082</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-14 04:49:30.224740</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-14 05:00:21.782844</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2022-11-14 05:11:47.573711</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2022-11-14 05:11:58.524342</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2022-11-14 05:12:59.443089</date>
    </status>
    <status>
      <message>Executing</message>
      <date>2023-04-07 11:08:36.088465</date>
    </status>
    <status>
      <message>Completed</message>
      <date>2023-04-07 11:08:51.780162</date>
    </status>
    <status>
      <message>Python-side metrics completed</message>
      <date>2023-04-07 11:17:55.915895</date>
    </status>
  </statuses>
  <results>
    <folds>
      <cv id="1">
        <metric name="NormalizedDCGEvaluator">0.01914778284570669</metric>
        <metric name="PStatisticalParityEvaluator">-0.4000000000000027</metric>
        <metric name="kendall_tau.py">-0.20179775280898876</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.37137297624001736</metric>
      </cv>
      <cv id="2">
        <metric name="NormalizedDCGEvaluator">0.02366553027241577</metric>
        <metric name="PStatisticalParityEvaluator">-0.4000000000000026</metric>
        <metric name="kendall_tau.py">-0.3586905257317243</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.3713172918772169</metric>
      </cv>
      <cv id="3">
        <metric name="NormalizedDCGEvaluator">0.025120854875001312</metric>
        <metric name="PStatisticalParityEvaluator">-0.4007828810020909</metric>
        <metric name="kendall_tau.py">-0.44757941462061307</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.3713095238095238</metric>
      </cv>
      <cv id="4">
        <metric name="NormalizedDCGEvaluator">0.02489682436335934</metric>
        <metric name="PStatisticalParityEvaluator">-0.4000000000000019</metric>
        <metric name="kendall_tau.py">-0.11813566375364128</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.37131479499831554</metric>
      </cv>
      <cv id="5">
        <metric name="NormalizedDCGEvaluator">0.022330791196567303</metric>
        <metric name="PStatisticalParityEvaluator">-0.400000000000002</metric>
        <metric name="kendall_tau.py">-0.05631294215563878</metric>
        <metric name="lowest_item_promoted.py">None</metric>
        <metric name="rank_biased_overlap.py">0.3721040663456394</metric>
      </cv>
    </folds>
    <averages/>
  </results>
</experiment>
